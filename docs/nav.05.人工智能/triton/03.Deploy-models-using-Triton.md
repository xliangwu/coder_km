# 03.部署一个模型

根据官方的文章：[Deploy models using Triton](https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_1-model_deployment/README.md)，深度学习服务推理主要有2块：
- Managing multiple models.（管理多个模型）
- 模型生命周期管理，包括模型版本、模型加载、模型卸载

首先clone仓库代码[tutorials](https://github.com/triton-inference-server/tutorials),接下来按如下步骤来体验：


1. 学习模型：
```bash
pip install onnx==1.14.0 -i https://pypi.douban.com/simple
```
2. 学习模型: Text Recognition

下载资源文件：
```bash
wget https://www.dropbox.com/sh/j3xmli4di1zuv3s/AABzCC1KGbIRe2wRwa3diWKwa/None-ResNet-None-CTC.pth
```

启动镜像 

*docker run -it --gpus all -v ${PWD}:/workspace nvcr.io/nvidia/pytorch:22.05-py3* ，执行下边的python代码。
```python
import torch
from utils.model import STRModel

# Create PyTorch Model Object
model = STRModel(input_channels=1, output_channels=512, num_classes=37)

# Load model weights from external file
state = torch.load("None-ResNet-None-CTC.pth")
state = {key.replace("module.", ""): value for key, value in state.items()}
model.load_state_dict(state)

# Create ONNX file by tracing model
trace_input = torch.randn(1, 1, 32, 100)
torch.onnx.export(model, trace_input, "str.onnx", verbose=True)
```
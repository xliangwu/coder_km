# 03.部署一个模型

根据官方的文章：[Deploy models using Triton](https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_1-model_deployment/README.md)，深度学习服务推理主要有2块：
- Managing multiple models.（管理多个模型）
- 模型生命周期管理，包括模型版本、模型加载、模型卸载

首先clone仓库代码[tutorials](https://github.com/triton-inference-server/tutorials),接下来按如下步骤来体验：


1. 学习模型：Text Detection
下载资源文件并转换
```bash
#download
wget https://www.dropbox.com/s/r2ingd0l3zt8hxs/frozen_east_text_detection.tar.gz
tar -xvf frozen_east_text_detection.tar.gz

#启动
docker run -it --gpus all -v ${PWD}:/workspace nvcr.io/nvidia/tensorflow:22.05-tf2-py3

#转换模型
pip install onnx==1.14.0 -i https://pypi.douban.com/simple
pip install -U tf2onnx
python -m tf2onnx.convert --input frozen_east_text_detection.pb --inputs "input_images:0" --outputs "feature_fusion/Conv_7/Sigmoid:0","feature_fusion/concat_3:0" --output detection.onnx

```
2. 学习模型: Text Recognition

下载资源文件：
```bash
wget https://www.dropbox.com/sh/j3xmli4di1zuv3s/AABzCC1KGbIRe2wRwa3diWKwa/None-ResNet-None-CTC.pth
```

启动镜像并生成模型文件

*docker run -it --gpus all -v ${PWD}:/workspace nvcr.io/nvidia/pytorch:22.05-py3* ，执行下边的python代码。
```python
import torch
from utils.model import STRModel

# Create PyTorch Model Object
model = STRModel(input_channels=1, output_channels=512, num_classes=37)

# Load model weights from external file
state = torch.load("None-ResNet-None-CTC.pth")
state = {key.replace("module.", ""): value for key, value in state.items()}
model.load_state_dict(state)

# Create ONNX file by tracing model
trace_input = torch.randn(1, 1, 32, 100)
torch.onnx.export(model, trace_input, "str.onnx", verbose=True)
```

3. 生成模型仓库
模型仓库是triton读取模型文件的目录结构。仓库可以是本地文件、也可以是远程仓库文件结构。
```sh
# Example repository structure
<model-repository>/
  <model-name>/
    [config.pbtxt]
    [<output-labels-file> ...]
    <version>/
      <model-definition-file>
    <version>/
      <model-definition-file>
    ...
  <model-name>/
    [config.pbtxt]
    [<output-labels-file> ...]
    <version>/
      <model-definition-file>
    <version>/
      <model-definition-file>
    ...
  
```

  上边步骤1和2已经生成好了模型文件，按照上边的结构生成模型仓库。
```bash
mkdir -p model_repository/text_detection/1
mv detection.onnx model_repository/text_detection/1/model.onnx

mkdir -p model_repository/text_recognition/1
mv str.onnx model_repository/text_recognition/1/model.onnx

#目录结构如下：
tree model_repository/

#output
model_repository/
|-- text_detection
|   |-- 1
|   |   `-- model.onnx
|   `-- config.pbtxt
`-- text_recognition
    |-- 1
    |   `-- model.onnx
    `-- config.pbtxt
```

4. 启动Triton，加载模型资源。
```bash
#通过docker启动，容器里边已经装好依赖的库。
docker run --gpus=all -it --shm-size=256m --rm -p8000:8000 -p8001:8001 -p8002:8002 -v $(pwd)/model_repository:/models nvcr.io/nvidia/tritonserver:22.05-py3

#启动
tritonserver --model-repository=/models
```

5. 客户端推理

triton 提供HTTP、GRPC方式做推理。可以通过Python、C等编写客户端进行交互。
```bash
pip install tritonclient[http] opencv-python-headless
python client.py

```